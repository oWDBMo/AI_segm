{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "image1 = torch.randn(1, 3, 256, 256)\n",
    "image2 = torch.randn(1, 3, 256, 256)\n",
    "example = [image1, image2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing APPANet model...\n",
      "Backbone: Resnet-101\n",
      "Number of classes: 2\n",
      "Output stride: 8\n",
      "Number of Input Channels: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\Temp\\ipykernel_30888\\3901899618.py:409: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = F.interpolate(x, size=(int(math.ceil(x_1.size()[-2] / 4)),\n",
      "X:\\Temp\\ipykernel_30888\\3901899618.py:410: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  int(math.ceil(x_1.size()[-1] / 4))), mode='bilinear', align_corners=True)\n",
      "D:\\Mysoftware_setup\\anaconda3\\envs\\mypytorch\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [2, 64, 128, 128]          18,816\n",
      "       BatchNorm2d-2          [2, 64, 128, 128]             128\n",
      "              ReLU-3          [2, 64, 128, 128]               0\n",
      "         MaxPool2d-4            [2, 64, 64, 64]               0\n",
      "            Conv2d-5            [2, 64, 64, 64]           4,096\n",
      "       BatchNorm2d-6            [2, 64, 64, 64]             128\n",
      "              ReLU-7            [2, 64, 64, 64]               0\n",
      "            Conv2d-8            [2, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9            [2, 64, 64, 64]             128\n",
      "             ReLU-10            [2, 64, 64, 64]               0\n",
      "           Conv2d-11           [2, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-12           [2, 256, 64, 64]             512\n",
      "           Conv2d-13           [2, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-14           [2, 256, 64, 64]             512\n",
      "             ReLU-15           [2, 256, 64, 64]               0\n",
      "       Bottleneck-16           [2, 256, 64, 64]               0\n",
      "           Conv2d-17            [2, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-18            [2, 64, 64, 64]             128\n",
      "             ReLU-19            [2, 64, 64, 64]               0\n",
      "           Conv2d-20            [2, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-21            [2, 64, 64, 64]             128\n",
      "             ReLU-22            [2, 64, 64, 64]               0\n",
      "           Conv2d-23           [2, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-24           [2, 256, 64, 64]             512\n",
      "             ReLU-25           [2, 256, 64, 64]               0\n",
      "       Bottleneck-26           [2, 256, 64, 64]               0\n",
      "           Conv2d-27            [2, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-28            [2, 64, 64, 64]             128\n",
      "             ReLU-29            [2, 64, 64, 64]               0\n",
      "           Conv2d-30            [2, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-31            [2, 64, 64, 64]             128\n",
      "             ReLU-32            [2, 64, 64, 64]               0\n",
      "           Conv2d-33           [2, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-34           [2, 256, 64, 64]             512\n",
      "             ReLU-35           [2, 256, 64, 64]               0\n",
      "       Bottleneck-36           [2, 256, 64, 64]               0\n",
      "           Conv2d-37           [2, 128, 64, 64]          32,768\n",
      "      BatchNorm2d-38           [2, 128, 64, 64]             256\n",
      "             ReLU-39           [2, 128, 64, 64]               0\n",
      "           Conv2d-40           [2, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-41           [2, 128, 32, 32]             256\n",
      "             ReLU-42           [2, 128, 32, 32]               0\n",
      "           Conv2d-43           [2, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-44           [2, 512, 32, 32]           1,024\n",
      "           Conv2d-45           [2, 512, 32, 32]         131,072\n",
      "      BatchNorm2d-46           [2, 512, 32, 32]           1,024\n",
      "             ReLU-47           [2, 512, 32, 32]               0\n",
      "       Bottleneck-48           [2, 512, 32, 32]               0\n",
      "           Conv2d-49           [2, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-50           [2, 128, 32, 32]             256\n",
      "             ReLU-51           [2, 128, 32, 32]               0\n",
      "           Conv2d-52           [2, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-53           [2, 128, 32, 32]             256\n",
      "             ReLU-54           [2, 128, 32, 32]               0\n",
      "           Conv2d-55           [2, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-56           [2, 512, 32, 32]           1,024\n",
      "             ReLU-57           [2, 512, 32, 32]               0\n",
      "       Bottleneck-58           [2, 512, 32, 32]               0\n",
      "           Conv2d-59           [2, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-60           [2, 128, 32, 32]             256\n",
      "             ReLU-61           [2, 128, 32, 32]               0\n",
      "           Conv2d-62           [2, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-63           [2, 128, 32, 32]             256\n",
      "             ReLU-64           [2, 128, 32, 32]               0\n",
      "           Conv2d-65           [2, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-66           [2, 512, 32, 32]           1,024\n",
      "             ReLU-67           [2, 512, 32, 32]               0\n",
      "       Bottleneck-68           [2, 512, 32, 32]               0\n",
      "           Conv2d-69           [2, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-70           [2, 128, 32, 32]             256\n",
      "             ReLU-71           [2, 128, 32, 32]               0\n",
      "           Conv2d-72           [2, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-73           [2, 128, 32, 32]             256\n",
      "             ReLU-74           [2, 128, 32, 32]               0\n",
      "           Conv2d-75           [2, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-76           [2, 512, 32, 32]           1,024\n",
      "             ReLU-77           [2, 512, 32, 32]               0\n",
      "       Bottleneck-78           [2, 512, 32, 32]               0\n",
      "           Conv2d-79           [2, 256, 32, 32]         131,072\n",
      "      BatchNorm2d-80           [2, 256, 32, 32]             512\n",
      "             ReLU-81           [2, 256, 32, 32]               0\n",
      "           Conv2d-82           [2, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-83           [2, 256, 32, 32]             512\n",
      "             ReLU-84           [2, 256, 32, 32]               0\n",
      "           Conv2d-85          [2, 1024, 32, 32]         262,144\n",
      "      BatchNorm2d-86          [2, 1024, 32, 32]           2,048\n",
      "           Conv2d-87          [2, 1024, 32, 32]         524,288\n",
      "      BatchNorm2d-88          [2, 1024, 32, 32]           2,048\n",
      "             ReLU-89          [2, 1024, 32, 32]               0\n",
      "       Bottleneck-90          [2, 1024, 32, 32]               0\n",
      "           Conv2d-91           [2, 256, 32, 32]         262,144\n",
      "      BatchNorm2d-92           [2, 256, 32, 32]             512\n",
      "             ReLU-93           [2, 256, 32, 32]               0\n",
      "           Conv2d-94           [2, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-95           [2, 256, 32, 32]             512\n",
      "             ReLU-96           [2, 256, 32, 32]               0\n",
      "           Conv2d-97          [2, 1024, 32, 32]         262,144\n",
      "      BatchNorm2d-98          [2, 1024, 32, 32]           2,048\n",
      "             ReLU-99          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-100          [2, 1024, 32, 32]               0\n",
      "          Conv2d-101           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-102           [2, 256, 32, 32]             512\n",
      "            ReLU-103           [2, 256, 32, 32]               0\n",
      "          Conv2d-104           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-105           [2, 256, 32, 32]             512\n",
      "            ReLU-106           [2, 256, 32, 32]               0\n",
      "          Conv2d-107          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-108          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-109          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-110          [2, 1024, 32, 32]               0\n",
      "          Conv2d-111           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-112           [2, 256, 32, 32]             512\n",
      "            ReLU-113           [2, 256, 32, 32]               0\n",
      "          Conv2d-114           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-115           [2, 256, 32, 32]             512\n",
      "            ReLU-116           [2, 256, 32, 32]               0\n",
      "          Conv2d-117          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-118          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-119          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-120          [2, 1024, 32, 32]               0\n",
      "          Conv2d-121           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-122           [2, 256, 32, 32]             512\n",
      "            ReLU-123           [2, 256, 32, 32]               0\n",
      "          Conv2d-124           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-125           [2, 256, 32, 32]             512\n",
      "            ReLU-126           [2, 256, 32, 32]               0\n",
      "          Conv2d-127          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-128          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-129          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-130          [2, 1024, 32, 32]               0\n",
      "          Conv2d-131           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-132           [2, 256, 32, 32]             512\n",
      "            ReLU-133           [2, 256, 32, 32]               0\n",
      "          Conv2d-134           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-135           [2, 256, 32, 32]             512\n",
      "            ReLU-136           [2, 256, 32, 32]               0\n",
      "          Conv2d-137          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-138          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-139          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-140          [2, 1024, 32, 32]               0\n",
      "          Conv2d-141           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-142           [2, 256, 32, 32]             512\n",
      "            ReLU-143           [2, 256, 32, 32]               0\n",
      "          Conv2d-144           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-145           [2, 256, 32, 32]             512\n",
      "            ReLU-146           [2, 256, 32, 32]               0\n",
      "          Conv2d-147          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-148          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-149          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-150          [2, 1024, 32, 32]               0\n",
      "          Conv2d-151           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-152           [2, 256, 32, 32]             512\n",
      "            ReLU-153           [2, 256, 32, 32]               0\n",
      "          Conv2d-154           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-155           [2, 256, 32, 32]             512\n",
      "            ReLU-156           [2, 256, 32, 32]               0\n",
      "          Conv2d-157          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-158          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-159          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-160          [2, 1024, 32, 32]               0\n",
      "          Conv2d-161           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-162           [2, 256, 32, 32]             512\n",
      "            ReLU-163           [2, 256, 32, 32]               0\n",
      "          Conv2d-164           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-165           [2, 256, 32, 32]             512\n",
      "            ReLU-166           [2, 256, 32, 32]               0\n",
      "          Conv2d-167          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-168          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-169          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-170          [2, 1024, 32, 32]               0\n",
      "          Conv2d-171           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-172           [2, 256, 32, 32]             512\n",
      "            ReLU-173           [2, 256, 32, 32]               0\n",
      "          Conv2d-174           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-175           [2, 256, 32, 32]             512\n",
      "            ReLU-176           [2, 256, 32, 32]               0\n",
      "          Conv2d-177          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-178          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-179          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-180          [2, 1024, 32, 32]               0\n",
      "          Conv2d-181           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-182           [2, 256, 32, 32]             512\n",
      "            ReLU-183           [2, 256, 32, 32]               0\n",
      "          Conv2d-184           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-185           [2, 256, 32, 32]             512\n",
      "            ReLU-186           [2, 256, 32, 32]               0\n",
      "          Conv2d-187          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-188          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-189          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-190          [2, 1024, 32, 32]               0\n",
      "          Conv2d-191           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-192           [2, 256, 32, 32]             512\n",
      "            ReLU-193           [2, 256, 32, 32]               0\n",
      "          Conv2d-194           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-195           [2, 256, 32, 32]             512\n",
      "            ReLU-196           [2, 256, 32, 32]               0\n",
      "          Conv2d-197          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-198          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-199          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-200          [2, 1024, 32, 32]               0\n",
      "          Conv2d-201           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-202           [2, 256, 32, 32]             512\n",
      "            ReLU-203           [2, 256, 32, 32]               0\n",
      "          Conv2d-204           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-205           [2, 256, 32, 32]             512\n",
      "            ReLU-206           [2, 256, 32, 32]               0\n",
      "          Conv2d-207          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-208          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-209          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-210          [2, 1024, 32, 32]               0\n",
      "          Conv2d-211           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-212           [2, 256, 32, 32]             512\n",
      "            ReLU-213           [2, 256, 32, 32]               0\n",
      "          Conv2d-214           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-215           [2, 256, 32, 32]             512\n",
      "            ReLU-216           [2, 256, 32, 32]               0\n",
      "          Conv2d-217          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-218          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-219          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-220          [2, 1024, 32, 32]               0\n",
      "          Conv2d-221           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-222           [2, 256, 32, 32]             512\n",
      "            ReLU-223           [2, 256, 32, 32]               0\n",
      "          Conv2d-224           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-225           [2, 256, 32, 32]             512\n",
      "            ReLU-226           [2, 256, 32, 32]               0\n",
      "          Conv2d-227          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-228          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-229          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-230          [2, 1024, 32, 32]               0\n",
      "          Conv2d-231           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-232           [2, 256, 32, 32]             512\n",
      "            ReLU-233           [2, 256, 32, 32]               0\n",
      "          Conv2d-234           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-235           [2, 256, 32, 32]             512\n",
      "            ReLU-236           [2, 256, 32, 32]               0\n",
      "          Conv2d-237          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-238          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-239          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-240          [2, 1024, 32, 32]               0\n",
      "          Conv2d-241           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-242           [2, 256, 32, 32]             512\n",
      "            ReLU-243           [2, 256, 32, 32]               0\n",
      "          Conv2d-244           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-245           [2, 256, 32, 32]             512\n",
      "            ReLU-246           [2, 256, 32, 32]               0\n",
      "          Conv2d-247          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-248          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-249          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-250          [2, 1024, 32, 32]               0\n",
      "          Conv2d-251           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-252           [2, 256, 32, 32]             512\n",
      "            ReLU-253           [2, 256, 32, 32]               0\n",
      "          Conv2d-254           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-255           [2, 256, 32, 32]             512\n",
      "            ReLU-256           [2, 256, 32, 32]               0\n",
      "          Conv2d-257          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-258          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-259          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-260          [2, 1024, 32, 32]               0\n",
      "          Conv2d-261           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-262           [2, 256, 32, 32]             512\n",
      "            ReLU-263           [2, 256, 32, 32]               0\n",
      "          Conv2d-264           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-265           [2, 256, 32, 32]             512\n",
      "            ReLU-266           [2, 256, 32, 32]               0\n",
      "          Conv2d-267          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-268          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-269          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-270          [2, 1024, 32, 32]               0\n",
      "          Conv2d-271           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-272           [2, 256, 32, 32]             512\n",
      "            ReLU-273           [2, 256, 32, 32]               0\n",
      "          Conv2d-274           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-275           [2, 256, 32, 32]             512\n",
      "            ReLU-276           [2, 256, 32, 32]               0\n",
      "          Conv2d-277          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-278          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-279          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-280          [2, 1024, 32, 32]               0\n",
      "          Conv2d-281           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-282           [2, 256, 32, 32]             512\n",
      "            ReLU-283           [2, 256, 32, 32]               0\n",
      "          Conv2d-284           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-285           [2, 256, 32, 32]             512\n",
      "            ReLU-286           [2, 256, 32, 32]               0\n",
      "          Conv2d-287          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-288          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-289          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-290          [2, 1024, 32, 32]               0\n",
      "          Conv2d-291           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-292           [2, 256, 32, 32]             512\n",
      "            ReLU-293           [2, 256, 32, 32]               0\n",
      "          Conv2d-294           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-295           [2, 256, 32, 32]             512\n",
      "            ReLU-296           [2, 256, 32, 32]               0\n",
      "          Conv2d-297          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-298          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-299          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-300          [2, 1024, 32, 32]               0\n",
      "          Conv2d-301           [2, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-302           [2, 256, 32, 32]             512\n",
      "            ReLU-303           [2, 256, 32, 32]               0\n",
      "          Conv2d-304           [2, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-305           [2, 256, 32, 32]             512\n",
      "            ReLU-306           [2, 256, 32, 32]               0\n",
      "          Conv2d-307          [2, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-308          [2, 1024, 32, 32]           2,048\n",
      "            ReLU-309          [2, 1024, 32, 32]               0\n",
      "      Bottleneck-310          [2, 1024, 32, 32]               0\n",
      "          Conv2d-311           [2, 512, 32, 32]         524,288\n",
      "     BatchNorm2d-312           [2, 512, 32, 32]           1,024\n",
      "            ReLU-313           [2, 512, 32, 32]               0\n",
      "          Conv2d-314           [2, 512, 32, 32]       2,359,296\n",
      "     BatchNorm2d-315           [2, 512, 32, 32]           1,024\n",
      "            ReLU-316           [2, 512, 32, 32]               0\n",
      "          Conv2d-317          [2, 2048, 32, 32]       1,048,576\n",
      "     BatchNorm2d-318          [2, 2048, 32, 32]           4,096\n",
      "          Conv2d-319          [2, 2048, 32, 32]       2,097,152\n",
      "     BatchNorm2d-320          [2, 2048, 32, 32]           4,096\n",
      "            ReLU-321          [2, 2048, 32, 32]               0\n",
      "      Bottleneck-322          [2, 2048, 32, 32]               0\n",
      "          Conv2d-323           [2, 512, 32, 32]       1,048,576\n",
      "     BatchNorm2d-324           [2, 512, 32, 32]           1,024\n",
      "            ReLU-325           [2, 512, 32, 32]               0\n",
      "          Conv2d-326           [2, 512, 32, 32]       2,359,296\n",
      "     BatchNorm2d-327           [2, 512, 32, 32]           1,024\n",
      "            ReLU-328           [2, 512, 32, 32]               0\n",
      "          Conv2d-329          [2, 2048, 32, 32]       1,048,576\n",
      "     BatchNorm2d-330          [2, 2048, 32, 32]           4,096\n",
      "            ReLU-331          [2, 2048, 32, 32]               0\n",
      "      Bottleneck-332          [2, 2048, 32, 32]               0\n",
      "          Conv2d-333           [2, 512, 32, 32]       1,048,576\n",
      "     BatchNorm2d-334           [2, 512, 32, 32]           1,024\n",
      "            ReLU-335           [2, 512, 32, 32]               0\n",
      "          Conv2d-336           [2, 512, 32, 32]       2,359,296\n",
      "     BatchNorm2d-337           [2, 512, 32, 32]           1,024\n",
      "            ReLU-338           [2, 512, 32, 32]               0\n",
      "          Conv2d-339          [2, 2048, 32, 32]       1,048,576\n",
      "     BatchNorm2d-340          [2, 2048, 32, 32]           4,096\n",
      "            ReLU-341          [2, 2048, 32, 32]               0\n",
      "      Bottleneck-342          [2, 2048, 32, 32]               0\n",
      "          ResNet-343  [[-1, 256, 64, 64], [-1, 512, 32, 32], [-1, 1024, 32, 32], [-1, 2048, 32, 32]]               0\n",
      "          Conv2d-344           [2, 512, 32, 32]       9,437,184\n",
      "     BatchNorm2d-345           [2, 512, 32, 32]           1,024\n",
      "            ReLU-346           [2, 512, 32, 32]               0\n",
      "          Conv2d-347           [2, 128, 32, 32]          65,536\n",
      "     BatchNorm2d-348           [2, 128, 32, 32]             256\n",
      "            ReLU-349           [2, 128, 32, 32]               0\n",
      "     ASPP_module-350           [2, 128, 32, 32]               0\n",
      "          Conv2d-351           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-352           [2, 128, 32, 32]             256\n",
      "            ReLU-353           [2, 128, 32, 32]               0\n",
      "     ASPP_module-354           [2, 128, 32, 32]               0\n",
      "          Conv2d-355           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-356           [2, 128, 32, 32]             256\n",
      "            ReLU-357           [2, 128, 32, 32]               0\n",
      "     ASPP_module-358           [2, 128, 32, 32]               0\n",
      "          Conv2d-359           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-360           [2, 128, 32, 32]             256\n",
      "            ReLU-361           [2, 128, 32, 32]               0\n",
      "     ASPP_module-362           [2, 128, 32, 32]               0\n",
      "AdaptiveAvgPool2d-363             [2, 512, 1, 1]               0\n",
      "          Conv2d-364             [2, 128, 1, 1]          65,536\n",
      "     BatchNorm2d-365             [2, 128, 1, 1]             256\n",
      "            ReLU-366             [2, 128, 1, 1]               0\n",
      "          Conv2d-367           [2, 128, 32, 32]          82,048\n",
      "          Conv2d-368           [2, 128, 32, 32]          65,536\n",
      "     BatchNorm2d-369           [2, 128, 32, 32]             256\n",
      "            ReLU-370           [2, 128, 32, 32]               0\n",
      "     ASPP_module-371           [2, 128, 32, 32]               0\n",
      "          Conv2d-372           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-373           [2, 128, 32, 32]             256\n",
      "            ReLU-374           [2, 128, 32, 32]               0\n",
      "     ASPP_module-375           [2, 128, 32, 32]               0\n",
      "          Conv2d-376           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-377           [2, 128, 32, 32]             256\n",
      "            ReLU-378           [2, 128, 32, 32]               0\n",
      "     ASPP_module-379           [2, 128, 32, 32]               0\n",
      "          Conv2d-380           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-381           [2, 128, 32, 32]             256\n",
      "            ReLU-382           [2, 128, 32, 32]               0\n",
      "     ASPP_module-383           [2, 128, 32, 32]               0\n",
      "AdaptiveAvgPool2d-384             [2, 512, 1, 1]               0\n",
      "          Conv2d-385             [2, 128, 1, 1]          65,536\n",
      "     BatchNorm2d-386             [2, 128, 1, 1]             256\n",
      "            ReLU-387             [2, 128, 1, 1]               0\n",
      "          Conv2d-388           [2, 128, 32, 32]          82,048\n",
      "         Softmax-389            [2, 1024, 1024]               0\n",
      "          Conv2d-390           [2, 512, 32, 32]         262,656\n",
      "           APPAP-391           [2, 512, 32, 32]               0\n",
      "          Conv2d-392           [2, 512, 32, 32]       2,359,296\n",
      "     BatchNorm2d-393           [2, 512, 32, 32]           1,024\n",
      "            ReLU-394           [2, 512, 32, 32]               0\n",
      "          Conv2d-395           [2, 512, 32, 32]       9,437,184\n",
      "     BatchNorm2d-396           [2, 512, 32, 32]           1,024\n",
      "            ReLU-397           [2, 512, 32, 32]               0\n",
      "          Conv2d-398           [2, 128, 32, 32]          65,536\n",
      "     BatchNorm2d-399           [2, 128, 32, 32]             256\n",
      "            ReLU-400           [2, 128, 32, 32]               0\n",
      "     ASPP_module-401           [2, 128, 32, 32]               0\n",
      "          Conv2d-402           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-403           [2, 128, 32, 32]             256\n",
      "            ReLU-404           [2, 128, 32, 32]               0\n",
      "     ASPP_module-405           [2, 128, 32, 32]               0\n",
      "          Conv2d-406           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-407           [2, 128, 32, 32]             256\n",
      "            ReLU-408           [2, 128, 32, 32]               0\n",
      "     ASPP_module-409           [2, 128, 32, 32]               0\n",
      "          Conv2d-410           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-411           [2, 128, 32, 32]             256\n",
      "            ReLU-412           [2, 128, 32, 32]               0\n",
      "     ASPP_module-413           [2, 128, 32, 32]               0\n",
      "AdaptiveAvgPool2d-414             [2, 512, 1, 1]               0\n",
      "          Conv2d-415             [2, 128, 1, 1]          65,536\n",
      "     BatchNorm2d-416             [2, 128, 1, 1]             256\n",
      "            ReLU-417             [2, 128, 1, 1]               0\n",
      "          Conv2d-418           [2, 512, 32, 32]         328,192\n",
      "          Conv2d-419           [2, 128, 32, 32]          65,536\n",
      "     BatchNorm2d-420           [2, 128, 32, 32]             256\n",
      "            ReLU-421           [2, 128, 32, 32]               0\n",
      "     ASPP_module-422           [2, 128, 32, 32]               0\n",
      "          Conv2d-423           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-424           [2, 128, 32, 32]             256\n",
      "            ReLU-425           [2, 128, 32, 32]               0\n",
      "     ASPP_module-426           [2, 128, 32, 32]               0\n",
      "          Conv2d-427           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-428           [2, 128, 32, 32]             256\n",
      "            ReLU-429           [2, 128, 32, 32]               0\n",
      "     ASPP_module-430           [2, 128, 32, 32]               0\n",
      "          Conv2d-431           [2, 128, 32, 32]         589,824\n",
      "     BatchNorm2d-432           [2, 128, 32, 32]             256\n",
      "            ReLU-433           [2, 128, 32, 32]               0\n",
      "     ASPP_module-434           [2, 128, 32, 32]               0\n",
      "AdaptiveAvgPool2d-435             [2, 512, 1, 1]               0\n",
      "          Conv2d-436             [2, 128, 1, 1]          65,536\n",
      "     BatchNorm2d-437             [2, 128, 1, 1]             256\n",
      "            ReLU-438             [2, 128, 1, 1]               0\n",
      "          Conv2d-439           [2, 512, 32, 32]         328,192\n",
      "         Softmax-440              [2, 512, 512]               0\n",
      "           APPAC-441           [2, 512, 32, 32]               0\n",
      "          Conv2d-442           [2, 512, 32, 32]       2,359,296\n",
      "     BatchNorm2d-443           [2, 512, 32, 32]           1,024\n",
      "            ReLU-444           [2, 512, 32, 32]               0\n",
      "        APPAHead-445           [2, 512, 32, 32]               0\n",
      "          Conv2d-446           [2, 256, 32, 32]         131,072\n",
      "     BatchNorm2d-447           [2, 256, 32, 32]             512\n",
      "            ReLU-448           [2, 256, 32, 32]               0\n",
      "          Conv2d-449            [2, 48, 64, 64]          12,288\n",
      "     BatchNorm2d-450            [2, 48, 64, 64]              96\n",
      "            ReLU-451            [2, 48, 64, 64]               0\n",
      "          Conv2d-452           [2, 256, 64, 64]         700,416\n",
      "     BatchNorm2d-453           [2, 256, 64, 64]             512\n",
      "            ReLU-454           [2, 256, 64, 64]               0\n",
      "          Conv2d-455           [2, 256, 64, 64]         589,824\n",
      "     BatchNorm2d-456           [2, 256, 64, 64]             512\n",
      "            ReLU-457           [2, 256, 64, 64]               0\n",
      "          Conv2d-458             [2, 2, 64, 64]             514\n",
      "================================================================\n",
      "Total params: 76,232,802\n",
      "Trainable params: 76,232,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3619.18\n",
      "Params size (MB): 290.81\n",
      "Estimated Total Size (MB): 3909.98\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, dilation=dilation, padding=dilation,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, nInputChannels, block, layers, os=16, pretrained=False):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        if os == 8:\n",
    "            strides = [1, 2, 1, 1]\n",
    "            dilations = [1, 1, 2, 2]\n",
    "            blocks = [1, 2, 1]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.conv1 = nn.Conv2d(nInputChannels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=strides[0], dilation=dilations[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=strides[1], dilation=dilations[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=strides[2], dilation=dilations[2])\n",
    "        self.layer4 = self._make_MG_unit(block, 512, blocks=blocks, stride=strides[3], dilation=dilations[3])\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "        if pretrained:\n",
    "            self._load_pretrained_model()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, dilation, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_MG_unit(self, block, planes, blocks=[1, 2, 4], stride=1, dilation=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, dilation=blocks[0] * dilation, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, len(blocks)):\n",
    "            layers.append(block(self.inplanes, planes, stride=1, dilation=blocks[i] * dilation))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        return x1, x2, x3, x4\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _load_pretrained_model(self):\n",
    "        pretrain_dict = model_zoo.load_url(url='https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "                                           model_dir='D:/Projects/Segmentation/pretrained')\n",
    "        model_dict = {}\n",
    "        state_dict = self.state_dict()\n",
    "        for k, v in pretrain_dict.items():\n",
    "            if k in state_dict:\n",
    "                model_dict[k] = v\n",
    "        state_dict.update(model_dict)\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def ResNet101(nInputChannels=3, os=8, pretrained=False):\n",
    "    model = ResNet(nInputChannels, Bottleneck, [3, 4, 23, 3], os, pretrained=pretrained)\n",
    "    return model\n",
    "\n",
    "\n",
    "class ASPP_module(nn.Module):\n",
    "    def __init__(self, inplanes, planes, dilation):\n",
    "        super(ASPP_module, self).__init__()\n",
    "        if dilation == 1:\n",
    "            kernel_size = 1\n",
    "            padding = 0\n",
    "        else:\n",
    "            kernel_size = 3\n",
    "            padding = dilation\n",
    "\n",
    "        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size, stride=1, padding=padding,\n",
    "                                     dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.atrous_conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            # torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class APPAP(nn.Module):\n",
    "    \"\"\" Position attention module\"\"\"\n",
    "\n",
    "    # Ref from SAGAN\n",
    "    def __init__(self, in_dim, os):\n",
    "        super(APPAP, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.os = os\n",
    "        self.query_conv = nn.Conv2d(in_channels=640, out_channels=128, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=640, out_channels=128, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=512, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        if self.os == 8:\n",
    "            dilations = [1, 2, 3, 6]\n",
    "        self.aspp1 = ASPP_module(inplanes=512, planes=128, dilation=dilations[0])\n",
    "        self.aspp2 = ASPP_module(inplanes=512, planes=128, dilation=dilations[1])\n",
    "        self.aspp3 = ASPP_module(inplanes=512, planes=128, dilation=dilations[2])\n",
    "        self.aspp4 = ASPP_module(inplanes=512, planes=128, dilation=dilations[3])\n",
    "        self.aspp5 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)), nn.Conv2d(512, 128, 1, stride=1, bias=False),\n",
    "                                   nn.BatchNorm2d(128), nn.ReLU())\n",
    "        self.conv = nn.Conv2d(in_channels=640, out_channels=512, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()  ### [2 512 32 32] m=2 C =  512 h = 32 w = 32\n",
    "        #proj_query1 = self.query_conv(x)\n",
    "        p_q1 = self.aspp1(x)\n",
    "        #print(p_q1.shape)           [2 128 32 32]\n",
    "        p_q2 = self.aspp2(x)\n",
    "        #print(p_q2.shape)           [2 128 32 32]\n",
    "        p_q3 = self.aspp3(x)\n",
    "        #print(p_q3.shape)           [2 128 32 32]\n",
    "        p_q4 = self.aspp4(x)\n",
    "        #print(p_q4.shape)           [2 128 32 32]\n",
    "        p_q5 = self.aspp5(x)\n",
    "        #print(p_q5.shape)           [2 128 1 1]\n",
    "        p_q5 = F.interpolate(p_q5, size=p_q4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        #print(p_q5.shape)           [2 128 32 32]\n",
    "        proj_query = torch.cat((p_q1, p_q2, p_q3, p_q4, p_q5), dim=1)\n",
    "        #print(proj_query.shape)      [2 640 32 32]\n",
    "        proj_query = self.query_conv(proj_query).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n",
    "        #print(proj_query.shape)      [2 1024 128]\n",
    "        #proj_key1 = self.key_conv(x)\n",
    "        p_k1 = self.aspp1(x)\n",
    "        p_k2 = self.aspp2(x)\n",
    "        p_k3 = self.aspp3(x)\n",
    "        p_k4 = self.aspp4(x)\n",
    "        p_k5 = self.aspp5(x)\n",
    "        p_k5 = F.interpolate(p_k5, size=p_k4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        proj_key = torch.cat((p_k1, p_k2, p_k3, p_k4, p_k5), dim=1)\n",
    "        proj_key = self.key_conv(proj_key).view(m_batchsize, -1, width * height)\n",
    "        #print(proj_key.shape) [2 128 1024]\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        #print(energy.shape) [2 1024 1024]\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n",
    "        #print(proj_value.shape) [2 512 1024]\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        #print(out.shape)  [2 512 1024]\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class APPAC(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, os):\n",
    "        super(APPAC, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.os = os\n",
    "        if self.os == 8:\n",
    "            dilations = [1, 2, 3, 6]\n",
    "\n",
    "        self.aspp1 = ASPP_module(inplanes=512, planes=128, dilation=dilations[0])\n",
    "        self.aspp2 = ASPP_module(inplanes=512, planes=128, dilation=dilations[1])\n",
    "        self.aspp3 = ASPP_module(inplanes=512, planes=128, dilation=dilations[2])\n",
    "        self.aspp4 = ASPP_module(inplanes=512, planes=128, dilation=dilations[3])\n",
    "        self.aspp5 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)), nn.Conv2d(512, 128, 1, stride=1, bias=False),\n",
    "                                   nn.BatchNorm2d(128), nn.ReLU())\n",
    "        self.conv1 = nn.Conv2d(in_channels=640, out_channels=512, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        #print(x.shape) [2 512 32 32]\n",
    "        p_q1 = self.aspp1(x)\n",
    "        p_q2 = self.aspp2(x)\n",
    "        p_q3 = self.aspp3(x)\n",
    "        p_q4 = self.aspp4(x)\n",
    "        p_q5 = self.aspp5(x)\n",
    "        p_q5 = F.interpolate(p_q5, size=p_q4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        proj_query = torch.cat((p_q1, p_q2, p_q3, p_q4, p_q5), dim=1)\n",
    "        proj_query = self.conv1(proj_query).view(m_batchsize, C, -1)\n",
    "        #print(proj_query.shape) [2 512 1024]\n",
    "\n",
    "        p_k1 = self.aspp1(x)\n",
    "        p_k2 = self.aspp2(x)\n",
    "        p_k3 = self.aspp3(x)\n",
    "        p_k4 = self.aspp4(x)\n",
    "        p_k5 = self.aspp5(x)\n",
    "        p_k5 = F.interpolate(p_k5, size=p_k4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        proj_key = torch.cat((p_k1, p_k2, p_k3, p_k4, p_k5), dim=1)\n",
    "        proj_key = self.conv1(proj_key).view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        #print(proj_key.shape)  [2 1024 612]\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        #print(energy.shape) [2 512 512]\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy) - energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "        #print(out.shape) [2 512 32 32]\n",
    "        return out\n",
    "\n",
    "\n",
    "class APPAHead(nn.Module):\n",
    "    def __init__(self, in_channels, norm_layer, os):\n",
    "        super(APPAHead, self).__init__()\n",
    "        inter_channels = in_channels // 4\n",
    "        self.conv5a = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                    norm_layer(inter_channels),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        self.conv5c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                    norm_layer(inter_channels),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        self.appap = APPAP(inter_channels, os)\n",
    "        self.appac = APPAC(inter_channels, os)\n",
    "        self.conv51 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                    norm_layer(inter_channels),\n",
    "                                    nn.ReLU())\n",
    "        self.conv52 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                    norm_layer(inter_channels),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat1 = self.conv5a(x)\n",
    "        ###print(feat1.shape) [2 512 32 32]\n",
    "        appap_feat = self.appap(feat1)\n",
    "        appap_conv = self.conv51(appap_feat)\n",
    "\n",
    "        feat2 = self.conv5c(x)\n",
    "        appac_feat = self.appac(feat2)\n",
    "        appac_conv = self.conv52(appac_feat)\n",
    "\n",
    "        feat_sum = appap_conv + appac_conv\n",
    "\n",
    "        return feat_sum\n",
    "\n",
    "\n",
    "class APPANet(nn.Module):\n",
    "    def __init__(self, nInputChannels=3, n_classes=2, os=8, aux=False, pretrained=False, _print=True):\n",
    "        if _print:\n",
    "            print(\"Constructing APPANet model...\")\n",
    "            print(\"Backbone: Resnet-101\")\n",
    "            print(\"Number of classes: {}\".format(n_classes))\n",
    "            print(\"Output stride: {}\".format(os))\n",
    "            print(\"Number of Input Channels: {}\".format(nInputChannels))\n",
    "        super(APPANet, self).__init__()\n",
    "        self.head = APPAHead(2048, nn.BatchNorm2d, os)\n",
    "        # Atrous Convolution\n",
    "        self.resnet_features = ResNet101(nInputChannels, os, pretrained=pretrained)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(512, 256, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "        # adopt [1x1, 48] for channel reduction\n",
    "        self.conv2 = nn.Conv2d(256, 48, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(48)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 48, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "\n",
    "        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(256, n_classes, kernel_size=1, stride=1))\n",
    "\n",
    "    def forward(self, x_1, x_2):\n",
    "        x = torch.cat((x_1, x_2), 1)\n",
    "        ###print(x.shape) [2 6 256 256]\n",
    "        x1, x2, x3, x4 = self.resnet_features(x)\n",
    "        ###print(x1.shape,x2.shape,x3.shape,x4.shape)[2 256 64 64] [2 512 32 32] [2 1024 32 32] [2 2048 32 32]\n",
    "\n",
    "        x_sum = self.head(x4)\n",
    "        #print(x_sum.shape)   [2 512 32 32]\n",
    "        x = self.conv1(x_sum)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)  [2 256 32 32]\n",
    "        x = F.interpolate(x, size=(int(math.ceil(x_1.size()[-2] / 4)),\n",
    "                                   int(math.ceil(x_1.size()[-1] / 4))), mode='bilinear', align_corners=True)\n",
    "        #print(x.shape) #[2 256 64 64]\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.bn2(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        #print(x1.shape)\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "\n",
    "        x = self.last_conv(x)\n",
    "        x = F.interpolate(x, size=x_1.size()[2:], mode='bilinear', align_corners=True)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        x = F.sigmoid(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model = APPANet(nInputChannels=6, n_classes=2, os=8, pretrained=False, _print=True)\n",
    "#     model.eval()\n",
    "#     image1 = torch.randn(1, 3, 256, 256)\n",
    "#     image2 = torch.randn(1, 3, 256, 256)\n",
    "#     with torch.no_grad():\n",
    "#         output = model.forward(image1, image2)\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "model = APPANet(nInputChannels=6, n_classes=2, os=8, pretrained=False, _print=True)\n",
    "writer.add_graph(model=model, input_to_model=example)\n",
    "summary(model, input_size=[(3, 256, 256), (3, 256, 256)], batch_size=2, device=\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [2, 64, 256, 256]          18,880\n",
      "       BatchNorm2d-2          [2, 64, 256, 256]             128\n",
      "              ReLU-3          [2, 64, 256, 256]               0\n",
      "         MaxPool2d-4          [2, 64, 128, 128]               0\n",
      "            Conv2d-5          [2, 64, 128, 128]         200,768\n",
      "       BatchNorm2d-6          [2, 64, 128, 128]             128\n",
      "              ReLU-7          [2, 64, 128, 128]               0\n",
      "         MaxPool2d-8            [2, 64, 64, 64]               0\n",
      "            Conv2d-9            [2, 64, 64, 64]         200,768\n",
      "      BatchNorm2d-10            [2, 64, 64, 64]             128\n",
      "             ReLU-11            [2, 64, 64, 64]               0\n",
      "        MaxPool2d-12            [2, 64, 32, 32]               0\n",
      "           Conv2d-13            [2, 64, 32, 32]         200,768\n",
      "      BatchNorm2d-14            [2, 64, 32, 32]             128\n",
      "             ReLU-15            [2, 64, 32, 32]               0\n",
      "        MaxPool2d-16            [2, 64, 16, 16]               0\n",
      "         Upsample-17            [2, 64, 32, 32]               0\n",
      "           Conv2d-18            [2, 64, 32, 32]         200,768\n",
      "      BatchNorm2d-19            [2, 64, 32, 32]             128\n",
      "             ReLU-20            [2, 64, 32, 32]               0\n",
      "         Upsample-21            [2, 64, 64, 64]               0\n",
      "           Conv2d-22            [2, 64, 64, 64]         200,768\n",
      "      BatchNorm2d-23            [2, 64, 64, 64]             128\n",
      "             ReLU-24            [2, 64, 64, 64]               0\n",
      "         Upsample-25          [2, 64, 128, 128]               0\n",
      "           Conv2d-26          [2, 64, 128, 128]         200,768\n",
      "      BatchNorm2d-27          [2, 64, 128, 128]             128\n",
      "             ReLU-28          [2, 64, 128, 128]               0\n",
      "         Upsample-29          [2, 64, 256, 256]               0\n",
      "           Conv2d-30          [2, 64, 256, 256]         200,768\n",
      "      BatchNorm2d-31          [2, 64, 256, 256]             128\n",
      "             ReLU-32          [2, 64, 256, 256]               0\n",
      "           Conv2d-33           [2, 2, 256, 256]             130\n",
      "================================================================\n",
      "Total params: 1,425,410\n",
      "Trainable params: 1,425,410\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 618.25\n",
      "Params size (MB): 5.44\n",
      "Estimated Total Size (MB): 623.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CDNet(nn.Module):\n",
    "    def __init__(self, in_ch=6, out_ch=2):\n",
    "        super(CDNet, self).__init__()\n",
    "        filters = 64\n",
    "        self.conv1 = nn.Conv2d(in_ch, filters, kernel_size=7, padding=3, stride=1)\n",
    "        self.bn = nn.BatchNorm2d(filters)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(filters, filters, kernel_size=7, padding=3, stride=1)\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.final = nn.Conv2d(filters, out_ch, kernel_size=1, stride=1)\n",
    "        #self.sigmod = nn.Sigmoid(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        x = self.pool(self.relu(self.bn(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn(self.conv2(x))))\n",
    "\n",
    "        x = self.relu(self.bn(self.conv2(self.up(x))))\n",
    "        x = self.relu(self.bn(self.conv2(self.up(x))))\n",
    "        x = self.relu(self.bn(self.conv2(self.up(x))))\n",
    "        x = self.relu(self.bn(self.conv2(self.up(x))))\n",
    "\n",
    "        x = self.final(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "model = CDNet(in_ch = 6,out_ch =2)\n",
    "writer.add_graph(model=model, input_to_model=example)\n",
    "summary(model,input_size=[(3,256,256),(3,256,256)],batch_size = 2, device=\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 32, 32])\n",
      "torch.Size([1, 2, 32, 32])\n",
      "torch.Size([1, 2, 32, 32])\n",
      "torch.Size([1, 2, 32, 32])\n",
      "torch.Size([1, 2, 32, 32])\n",
      "torch.Size([1, 2, 32, 32])\n",
      "torch.Size([2, 2, 32, 32])\n",
      "torch.Size([2, 2, 32, 32])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [2, 64, 256, 256]           3,520\n",
      "              ReLU-2          [2, 64, 256, 256]               0\n",
      "            Conv2d-3          [2, 64, 256, 256]          36,928\n",
      "              ReLU-4          [2, 64, 256, 256]               0\n",
      "         MaxPool2d-5          [2, 64, 128, 128]               0\n",
      "            Conv2d-6         [2, 128, 128, 128]          73,856\n",
      "              ReLU-7         [2, 128, 128, 128]               0\n",
      "            Conv2d-8         [2, 128, 128, 128]         147,584\n",
      "              ReLU-9         [2, 128, 128, 128]               0\n",
      "        MaxPool2d-10           [2, 128, 64, 64]               0\n",
      "           Conv2d-11           [2, 256, 64, 64]         295,168\n",
      "             ReLU-12           [2, 256, 64, 64]               0\n",
      "           Conv2d-13           [2, 256, 64, 64]         590,080\n",
      "             ReLU-14           [2, 256, 64, 64]               0\n",
      "        MaxPool2d-15           [2, 256, 32, 32]               0\n",
      "           Conv2d-16           [2, 512, 32, 32]       1,180,160\n",
      "             ReLU-17           [2, 512, 32, 32]               0\n",
      "           Conv2d-18           [2, 512, 32, 32]       2,359,808\n",
      "             ReLU-19           [2, 512, 32, 32]               0\n",
      "        MaxPool2d-20           [2, 512, 16, 16]               0\n",
      "           Conv2d-21           [2, 512, 16, 16]       2,359,808\n",
      "             ReLU-22           [2, 512, 16, 16]               0\n",
      "           Conv2d-23           [2, 512, 16, 16]       2,359,808\n",
      "             ReLU-24           [2, 512, 16, 16]               0\n",
      "        MaxPool2d-25             [2, 512, 8, 8]               0\n",
      "           Conv2d-26            [2, 4096, 8, 8]     102,764,544\n",
      "             ReLU-27            [2, 4096, 8, 8]               0\n",
      "          Dropout-28            [2, 4096, 8, 8]               0\n",
      "           Conv2d-29            [2, 4096, 8, 8]      16,781,312\n",
      "             ReLU-30            [2, 4096, 8, 8]               0\n",
      "          Dropout-31            [2, 4096, 8, 8]               0\n",
      "           Conv2d-32               [2, 2, 8, 8]           8,194\n",
      "  ConvTranspose2d-33             [2, 2, 16, 16]              38\n",
      "           Conv2d-34             [2, 2, 16, 16]           1,026\n",
      "  ConvTranspose2d-35             [2, 2, 32, 32]              66\n",
      "           Conv2d-36             [2, 2, 32, 32]           4,610\n",
      "  ConvTranspose2d-37           [2, 2, 256, 256]             258\n",
      "          Softmax-38           [2, 2, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 128,966,768\n",
      "Trainable params: 128,966,768\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 546.58\n",
      "Params size (MB): 491.97\n",
      "Estimated Total Size (MB): 1038.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FCN_CD(nn.Module):\n",
    "    def __init__(self, in_ch=6, out_ch=2):\n",
    "        super(FCN_CD, self).__init__()\n",
    "        filters = [64, 128, 256, 512, 4096]\n",
    "        self.conv1 = nn.Conv2d(in_ch, filters[0], kernel_size=3, padding=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.conv2 = nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(filters[0], filters[1], kernel_size=3, padding=1, stride=1)\n",
    "        self.conv4 = nn.Conv2d(filters[1], filters[1], kernel_size=3, padding=1, stride=1)\n",
    "        self.conv5 = nn.Conv2d(filters[1], filters[2], kernel_size=3, padding=1, stride=1)\n",
    "        self.conv6 = nn.Conv2d(filters[2], filters[2], kernel_size=3, padding=1, stride=1)\n",
    "        self.conv7 = nn.Conv2d(filters[2], filters[3], kernel_size=3, padding=1, stride=1)\n",
    "        self.conv8 = nn.Conv2d(filters[3], filters[3], kernel_size=3, padding=1, stride=1)\n",
    "        self.conv9 = nn.Conv2d(filters[3], filters[4], kernel_size=7, padding=3, stride=1)\n",
    "        self.conv10 = nn.Conv2d(filters[4], filters[4], kernel_size=1, stride=1)\n",
    "        self.conv11 = nn.Conv2d(filters[4], out_ch, kernel_size=1, stride=1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv12 = nn.Conv2d(filters[3], out_ch, kernel_size=1, padding=0, stride=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=4, padding=1, stride=2)\n",
    "        self.conv13 = nn.Conv2d(filters[2], out_ch, kernel_size=3, padding=1, stride=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=8, stride=8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        #print(x.shape)\n",
    "        x = self.pool(self.relu(self.conv2(self.relu(self.conv1(x)))))\n",
    "        #print(x.shape)\n",
    "        x = self.pool(self.relu(self.conv4(self.relu(self.conv3(x)))))\n",
    "        #print(x.shape)\n",
    "        x1 = self.pool(self.relu(self.conv6(self.relu(self.conv5(x)))))\n",
    "        #print(x1.shape)\n",
    "        x2 = self.pool(self.relu(self.conv8(self.relu(self.conv7(x1)))))\n",
    "        #print(x2.shape)\n",
    "        x3 = self.pool(self.relu(self.conv8(self.relu(self.conv8(x2)))))\n",
    "        #print(x3.shape)\n",
    "        x4 = self.dropout(self.relu(self.conv9(x3)))\n",
    "        #print(x4.shape)\n",
    "        x5 = self.dropout(self.relu(self.conv10(x4)))\n",
    "        #print(x5.shape)\n",
    "        x6 = self.deconv1(self.conv11(x5))\n",
    "        #print(x6.shape)\n",
    "        x7 = self.conv12(x2)\n",
    "        #print(x7.shape)\n",
    "        x8 = self.deconv2(x7 + x6)\n",
    "        print(x8.shape)\n",
    "        x9 = self.conv13(x1)\n",
    "        print(x9.shape)\n",
    "        x10 = self.deconv3(x8 + x9)\n",
    "        final = self.softmax(x10)\n",
    "\n",
    "        return final\n",
    "\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "model = FCN_CD(in_ch = 6,out_ch =2)\n",
    "writer.add_graph(model=model, input_to_model=example)\n",
    "summary(model,input_size=[(3,256,256),(3,256,256)],batch_size = 2, device=\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class DCM(nn.Module):\n",
    "    def __init__(self, in_C, out_C):\n",
    "        super(DCM, self).__init__()\n",
    "        self.ks = [1, 3, 5]\n",
    "        if in_C == 2048:\n",
    "            self.mid_C = in_C // 4\n",
    "        if in_C == 1024:\n",
    "            self.mid_C = in_C // 2\n",
    "        if in_C == 512:\n",
    "            self.mid_C = in_C\n",
    "        if in_C == 256:\n",
    "            self.mid_C = in_C\n",
    "        if in_C == 128:\n",
    "            self.mid_C = in_C\n",
    "        if in_C == 64:\n",
    "            self.mid_C = in_C\n",
    "        self.ger_kernel_branches = nn.ModuleList()\n",
    "        for k in self.ks:\n",
    "            self.ger_kernel_branches.append(\n",
    "                nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d(k),\n",
    "                    nn.Conv2d(in_C, self.mid_C, kernel_size=1)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.trans_branches = nn.ModuleList()\n",
    "        self.fuse_inside_branches = nn.ModuleList()\n",
    "        for i in range(len(self.ks)):\n",
    "            self.trans_branches.append(\n",
    "                nn.Conv2d(in_C, self.mid_C, kernel_size=1)\n",
    "            )\n",
    "            self.fuse_inside_branches.append(\n",
    "                nn.Conv2d(self.mid_C, self.mid_C, 1)\n",
    "            )\n",
    "\n",
    "        self.fuse_outside = nn.Conv2d(len(self.ks) * self.mid_C + in_C, out_C, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x: 被卷积的特征\n",
    "        y: 用来生成卷积核\n",
    "        \"\"\"\n",
    "        feats_branches = [x]\n",
    "        for i in range(len(self.ks)):\n",
    "            kernel = self.ger_kernel_branches[i](y)\n",
    "            kernel_single = kernel.split(1, dim=0)\n",
    "            x_inside = self.trans_branches[i](x)\n",
    "            x_inside_single = x_inside.split(1, dim=0)\n",
    "            feat_single = []\n",
    "            for kernel_single_item, x_inside_single_item \\\n",
    "                    in zip(kernel_single, x_inside_single):\n",
    "                feat_inside_single = self.fuse_inside_branches[i](\n",
    "                    F.conv2d(\n",
    "                        x_inside_single_item,\n",
    "                        weight=kernel_single_item.transpose(0, 1),\n",
    "                        bias=None,\n",
    "                        stride=1,\n",
    "                        padding=self.ks[i] // 2,\n",
    "                        dilation=1,\n",
    "                        groups=self.mid_C\n",
    "                    )\n",
    "                )\n",
    "                feat_single.append(feat_inside_single)\n",
    "            feat_single = torch.cat(feat_single, dim=0)\n",
    "            feats_branches.append(feat_single)\n",
    "        return self.fuse_outside(torch.cat(feats_branches, dim=1))\n",
    "\n",
    "\n",
    "class ContextBlock(nn.Module):\n",
    "    def __init__(self, inplanes, ratio, pooling_type='att',\n",
    "                 fusion_types=('channel_add',)):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "        return out\n",
    "\n",
    "\n",
    "class MSAANet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, block, num_block, _print=True):\n",
    "        super(MSAANet, self).__init__()\n",
    "        if _print:\n",
    "            print(\"Constructing MSAANet model...\")\n",
    "            print(\"Backbone: Resnet-101\")\n",
    "            print(\"Number of classes: {}\".format(out_channel))\n",
    "            print(\"Number of Input Channels: {}\".format(in_channel))\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # we use a different inputsize than the original paper\n",
    "        # so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 2)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(3072, 1024, 1)\n",
    "        self.dconv_up3 = double_conv(1024, 512)\n",
    "        self.conv_2 = nn.Conv2d(1024, 512, 1)\n",
    "        self.dconv_up2 = double_conv(512, 256)\n",
    "        self.conv_3 = nn.Conv2d(512, 256, 1)\n",
    "        self.dconv_up1 = double_conv(256, 128)\n",
    "        #self.dconv_up0 = double_conv(192, 128)\n",
    "\n",
    "        self.dconv_last = nn.Sequential(\n",
    "            nn.Conv2d(192, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, out_channel, 1)\n",
    "        )\n",
    "        self.cb1 = ContextBlock(inplanes=64, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm1 = DCM(in_C=64, out_C=64)\n",
    "        self.cb2 = ContextBlock(inplanes=256, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm2 = DCM(in_C=256, out_C=256)\n",
    "        self.cb3 = ContextBlock(inplanes=512, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm3 = DCM(in_C=512, out_C=512)\n",
    "        self.cb4 = ContextBlock(inplanes=1024, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm4 = DCM(in_C=1024, out_C=1024)\n",
    "        self.cb5 = ContextBlock(inplanes=2048, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm5 = DCM(in_C=2048, out_C=2048)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        ###print(x.shape) [1 6 256 256]\n",
    "        conv1 = self.conv1(x)\n",
    "        ###print(conv1.shape) [1 64 256 256]\n",
    "        temp = self.maxpool(conv1)\n",
    "        ###print(temp.shape)  [1 64 128 128]  stage1\n",
    "        temp = self.dcm1(temp, temp)\n",
    "        temp = self.cb1(temp)\n",
    "        #print(temp.shape)\n",
    "        conv2 = self.conv2_x(temp)\n",
    "        ###print(conv2.shape) [1 256 64 64]   stage2\n",
    "        conv2 = self.dcm2(conv2, conv2)\n",
    "        conv2 = self.cb2(conv2)\n",
    "        conv3 = self.conv3_x(conv2)\n",
    "        ###print(conv3.shape) [1 512 32 32]   stage3\n",
    "        conv3 = self.dcm3(conv3, conv3)\n",
    "        conv3 = self.cb3(conv3)\n",
    "        conv4 = self.conv4_x(conv3)\n",
    "        ###print(conv4.shape) [1 1024 16 16]  stage4\n",
    "        conv4 = self.dcm4(conv4, conv4)\n",
    "        conv4 = self.cb4(conv4)\n",
    "        bottle = self.conv5_x(conv4)\n",
    "        ###print(bottle.shape) [1 2048 8 8]   stage5\n",
    "        # output = self.avg_pool(output)\n",
    "        # output = output.view(output.size(0), -1)\n",
    "        # output = self.fc(output)\n",
    "        bottle = self.dcm5(bottle, bottle)\n",
    "        bottle = self.cb5(bottle)\n",
    "        x = self.upsample(bottle)\n",
    "        ###print(x.shape)  [1 2048 16 16]\n",
    "        # print(x.shape)\n",
    "        # print(conv4.shape)\n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "        ###print(x.shape)  [1 3072 16 16]\n",
    "        x = self.conv_1(x)  ###[1 1024 16 16]\n",
    "        x = self.dconv_up3(x)  ###[1 512 16 16]\n",
    "        ###print(x.shape)###\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        ###print(x.shape)   [1 512 32 32]\n",
    "        # print(x.shape)\n",
    "        # print(conv3.shape) dim=1)###[1 1024 32 32]\n",
    "        x = self.conv_2(x)  ###[1 512 32 32]\n",
    "        x = self.dconv_up2(x)  ###[1 256 32 32]\n",
    "        x = self.upsample(x)  ###[1 256 64 64]\n",
    "        x = torch.cat([x, conv2], dim=1)  ###[1 512 64 64]\n",
    "        x = self.conv_3(x)\n",
    "        x = self.dconv_up1(x)\n",
    "        x = self.upsample(x)  ###[1 128 128 128]\n",
    "        x = torch.cat([x, temp], dim=1)  ###[1 192 128 128]\n",
    "        out = self.dconv_last(x)\n",
    "        #print(out.shape)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def Get_MSAANet(in_channel=6, out_channel=2):\n",
    "    return MSAANet(in_channel, out_channel, block=BottleNeck, num_block=[3, 4, 23, 3], _print=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = Get_MSAANet(6, 2)\n",
    "    model.eval()\n",
    "    image1 = torch.randn(1, 3, 256, 256)\n",
    "    image2 = torch.randn(1, 3, 256, 256)\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(image1, image2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class DCM(nn.Module):\n",
    "    def __init__(self, in_C, out_C):\n",
    "        super(DCM, self).__init__()\n",
    "        self.ks = [1, 3, 5]\n",
    "        if in_C == 2048:\n",
    "            self.mid_C = in_C // 4\n",
    "        if in_C == 1024:\n",
    "            self.mid_C = in_C // 2\n",
    "        if in_C == 512:\n",
    "            self.mid_C = in_C\n",
    "        if in_C == 256:\n",
    "            self.mid_C = in_C\n",
    "        if in_C == 128:\n",
    "            self.mid_C = in_C\n",
    "        if in_C == 64:\n",
    "            self.mid_C = in_C\n",
    "        self.ger_kernel_branches = nn.ModuleList()\n",
    "        for k in self.ks:\n",
    "            self.ger_kernel_branches.append(\n",
    "                nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d(k),\n",
    "                    nn.Conv2d(in_C, self.mid_C, kernel_size=1)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.trans_branches = nn.ModuleList()\n",
    "        self.fuse_inside_branches = nn.ModuleList()\n",
    "        for i in range(len(self.ks)):\n",
    "            self.trans_branches.append(\n",
    "                nn.Conv2d(in_C, self.mid_C, kernel_size=1)\n",
    "            )\n",
    "            self.fuse_inside_branches.append(\n",
    "                nn.Conv2d(self.mid_C, self.mid_C, 1)\n",
    "            )\n",
    "\n",
    "        self.fuse_outside = nn.Conv2d(len(self.ks) * self.mid_C + in_C, out_C, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x: 被卷积的特征\n",
    "        y: 用来生成卷积核\n",
    "        \"\"\"\n",
    "        feats_branches = [x]\n",
    "        for i in range(len(self.ks)):\n",
    "            kernel = self.ger_kernel_branches[i](y)\n",
    "            kernel_single = kernel.split(1, dim=0)\n",
    "            x_inside = self.trans_branches[i](x)\n",
    "            x_inside_single = x_inside.split(1, dim=0)\n",
    "            feat_single = []\n",
    "            for kernel_single_item, x_inside_single_item \\\n",
    "                    in zip(kernel_single, x_inside_single):\n",
    "                feat_inside_single = self.fuse_inside_branches[i](\n",
    "                    F.conv2d(\n",
    "                        x_inside_single_item,\n",
    "                        weight=kernel_single_item.transpose(0, 1),\n",
    "                        bias=None,\n",
    "                        stride=1,\n",
    "                        padding=self.ks[i] // 2,\n",
    "                        dilation=1,\n",
    "                        groups=self.mid_C\n",
    "                    )\n",
    "                )\n",
    "                feat_single.append(feat_inside_single)\n",
    "            feat_single = torch.cat(feat_single, dim=0)\n",
    "            feats_branches.append(feat_single)\n",
    "        return self.fuse_outside(torch.cat(feats_branches, dim=1))\n",
    "        #return torch.cat((fuse_outside,x),dim=1)\n",
    "\n",
    "\n",
    "class ContextBlock(nn.Module):\n",
    "    def __init__(self, inplanes, ratio, pooling_type='att',\n",
    "                 fusion_types=('channel_add',)):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "        return context\n",
    "\n",
    "    def forward(self, x, x1):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "        out = torch.cat((out, x1), dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MSAANet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, block, num_block):\n",
    "        super(MSAANet, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # we use a different inputsize than the original paper\n",
    "        # so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 2)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(3584, 2048, 1)\n",
    "        self.dconv_up3 = double_conv(2048, 1024)\n",
    "        self.conv_2 = nn.Conv2d(1536, 512, 1)\n",
    "        self.dconv_up2 = double_conv(512, 256)\n",
    "        self.conv_3 = nn.Conv2d(512, 256, 1)\n",
    "        self.dconv_up1 = double_conv(256, 128)\n",
    "        #self.dconv_up0 = double_conv(192, 128)\n",
    "\n",
    "        self.dconv_last = nn.Sequential(\n",
    "            nn.Conv2d(192, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, out_channel, 1)\n",
    "        )\n",
    "        self.cb1 = ContextBlock(inplanes=64, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm1 = DCM(in_C=64, out_C=64)\n",
    "        self.cb2 = ContextBlock(inplanes=256, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm2 = DCM(in_C=256, out_C=256)\n",
    "        self.cb3 = ContextBlock(inplanes=512, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm3 = DCM(in_C=512, out_C=512)\n",
    "        self.cb4 = ContextBlock(inplanes=512, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm4 = DCM(in_C=1024, out_C=512)\n",
    "        self.cb5 = ContextBlock(inplanes=512, ratio=1. / 16., pooling_type='att')\n",
    "        self.dcm5 = DCM(in_C=2048, out_C=512)\n",
    "\n",
    "        self.conv1_1x = nn.Conv2d(128, 64, 1)\n",
    "        self.conv2_1x = nn.Conv2d(512, 256, 1)\n",
    "        self.conv3_1x = nn.Conv2d(1024, 512, 1)\n",
    "        self.conv4_1x = nn.Conv2d(1536, 1024, 1)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        ###print(x.shape) [1 6 256 256]\n",
    "        conv1 = self.conv1(x)\n",
    "        ###print(conv1.shape) [1 64 256 256]\n",
    "        temp = self.maxpool(conv1)\n",
    "        temp_1 = temp\n",
    "        ###print(temp.shape)  [1 64 128 128]  stage1\n",
    "        temp = self.dcm1(temp, temp)\n",
    "        temp = self.cb1(temp, temp_1)\n",
    "        #print(temp.shape)\n",
    "        temp = self.conv1_1x(temp)\n",
    "        conv2 = self.conv2_x(temp)\n",
    "        ###print(conv2.shape)##\n",
    "        ###print(conv2.shape) [1 256 64 64]   stage2\n",
    "        conv2_1 = conv2\n",
    "        conv2 = self.dcm2(conv2, conv2)\n",
    "        conv2 = self.cb2(conv2, conv2_1)\n",
    "        ##print(conv2.shape)[1 512 64 64]\n",
    "        conv2 = self.conv2_1x(conv2)\n",
    "        conv3 = self.conv3_x(conv2)\n",
    "        ###print(conv3.shape) ###[1 512 32 32]   stage3\n",
    "        conv3_1 = conv3\n",
    "        conv3 = self.dcm3(conv3, conv3)\n",
    "        conv3 = self.cb3(conv3, conv3_1)\n",
    "        conv3 = self.conv3_1x(conv3)\n",
    "        conv4 = self.conv4_x(conv3)\n",
    "        ##print(conv4.shape) ###[1 1024 16 16]  stage4\n",
    "        conv4_1 = conv4\n",
    "        conv4 = self.dcm4(conv4, conv4)\n",
    "        ###print(conv4.shape)\n",
    "        conv4 = self.cb4(conv4, conv4_1)\n",
    "        ##print(conv4.shape)##[1 1536 16 16]\n",
    "        conv4 = self.conv4_1x(conv4)\n",
    "        bottle = self.conv5_x(conv4)\n",
    "        ###print(bottle.shape) [1 2048 8 8]   stage5\n",
    "        # output = self.avg_pool(output)\n",
    "        # output = output.view(output.size(0), -1)\n",
    "        # output = self.fc(output)\n",
    "        botttle_1 = bottle\n",
    "        bottle = self.dcm5(bottle, bottle)\n",
    "        bottle = self.cb5(bottle, botttle_1)\n",
    "        x = self.upsample(bottle)\n",
    "        ###print(x.shape)  [1 2048 16 16]\n",
    "        # print(x.shape)\n",
    "        # print(conv4.shape)\n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "        ###print(x.shape)  [1 3072 16 16]\n",
    "        x = self.conv_1(x)  ###[1 1024 16 16]\n",
    "        x = self.dconv_up3(x)  ###[1 512 16 16]\n",
    "        ###print(x.shape)###\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        ###print(x.shape)   [1 512 32 32]\n",
    "        # print(x.shape)\n",
    "        # print(conv3.shape) dim=1)###[1 1024 32 32]\n",
    "        x = self.conv_2(x)  ###[1 512 32 32]\n",
    "        x = self.dconv_up2(x)  ###[1 256 32 32]\n",
    "        x = self.upsample(x)  ###[1 256 64 64]\n",
    "        x = torch.cat([x, conv2], dim=1)  ###[1 512 64 64]\n",
    "        x = self.conv_3(x)\n",
    "        x = self.dconv_up1(x)\n",
    "        x = self.upsample(x)  ###[1 128 128 128]\n",
    "        x = torch.cat([x, temp], dim=1)  ###[1 192 128 128]\n",
    "        out = self.dconv_last(x)\n",
    "        #print(out.shape)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def Get_MSAANet(in_channel=6, out_channel=2):\n",
    "    return MSAANet(in_channel, out_channel, block=BottleNeck, num_block=[3, 4, 23, 3])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = Get_MSAANet(6, 2)\n",
    "    model.eval()\n",
    "    image1 = torch.randn(1, 3, 256, 256)\n",
    "    image2 = torch.randn(1, 3, 256, 256)\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(image1, image2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class conv_block_nested(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super(conv_block_nested, self).__init__()\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class NestedUNet_CD(nn.Module):\n",
    "    def __init__(self, in_ch=6, out_ch=2):\n",
    "        super(NestedUNet_CD, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
    "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
    "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
    "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
    "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
    "\n",
    "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])\n",
    "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
    "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
    "        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n",
    "\n",
    "        self.conv0_2 = conv_block_nested(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_2 = conv_block_nested(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
    "        self.conv2_2 = conv_block_nested(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
    "\n",
    "        self.conv0_3 = conv_block_nested(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_3 = conv_block_nested(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
    "\n",
    "        self.conv0_4 = conv_block_nested(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
    "\n",
    "        self.final1 = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.final2 = nn.Conv2d(out_ch * 4, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        x0_0 = self.conv0_0(x)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))\n",
    "\n",
    "        x0_1 = self.final1(x0_1)\n",
    "        y0_1 = self.sigmoid(x0_1)\n",
    "        x0_2 = self.final1(x0_2)\n",
    "        y0_2 = self.sigmoid(x0_2)\n",
    "        x0_3 = self.final1(x0_3)\n",
    "        y0_3 = self.sigmoid(x0_3)\n",
    "        x0_4 = self.final1(x0_4)\n",
    "        y0_4 = self.sigmoid(x0_4)\n",
    "\n",
    "        y0_5 = self.sigmoid(self.final2(torch.cat([y0_1, y0_2, y0_3, y0_4], 1)))\n",
    "\n",
    "        return y0_5\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from torchsummary import summary\n",
    "model = NestedUNet_CD(in_ch = 6,out_ch =2)\n",
    "summary(model,input_size=[(3,256,256),(3,256,256)],batch_size = 2, device=\"cpu\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.padding import ReplicationPad2d\n",
    "\n",
    "\n",
    "class SiamUnet_conc(nn.Module):\n",
    "    \"\"\"SiamUnet_conc segmentation network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_nbr, label_nbr):\n",
    "        super(SiamUnet_conc, self).__init__()\n",
    "\n",
    "        self.input_nbr = input_nbr\n",
    "\n",
    "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(16)\n",
    "        self.do11 = nn.Dropout2d(p=0.2)\n",
    "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(16)\n",
    "        self.do12 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(32)\n",
    "        self.do21 = nn.Dropout2d(p=0.2)\n",
    "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(32)\n",
    "        self.do22 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(64)\n",
    "        self.do31 = nn.Dropout2d(p=0.2)\n",
    "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(64)\n",
    "        self.do32 = nn.Dropout2d(p=0.2)\n",
    "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn33 = nn.BatchNorm2d(64)\n",
    "        self.do33 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(128)\n",
    "        self.do41 = nn.Dropout2d(p=0.2)\n",
    "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(128)\n",
    "        self.do42 = nn.Dropout2d(p=0.2)\n",
    "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn43 = nn.BatchNorm2d(128)\n",
    "        self.do43 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv43d = nn.ConvTranspose2d(384, 128, kernel_size=3, padding=1)\n",
    "        self.bn43d = nn.BatchNorm2d(128)\n",
    "        self.do43d = nn.Dropout2d(p=0.2)\n",
    "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(128)\n",
    "        self.do42d = nn.Dropout2d(p=0.2)\n",
    "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(64)\n",
    "        self.do41d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv33d = nn.ConvTranspose2d(192, 64, kernel_size=3, padding=1)\n",
    "        self.bn33d = nn.BatchNorm2d(64)\n",
    "        self.do33d = nn.Dropout2d(p=0.2)\n",
    "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(64)\n",
    "        self.do32d = nn.Dropout2d(p=0.2)\n",
    "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(32)\n",
    "        self.do31d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv22d = nn.ConvTranspose2d(96, 32, kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(32)\n",
    "        self.do22d = nn.Dropout2d(p=0.2)\n",
    "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(16)\n",
    "        self.do21d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv12d = nn.ConvTranspose2d(48, 16, kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(16)\n",
    "        self.do12d = nn.Dropout2d(p=0.2)\n",
    "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
    "\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"Forward method.\"\"\"\n",
    "        # Stage 1\n",
    "        x11_1 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
    "        #print(x11.shape)\n",
    "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11_1))))\n",
    "        #print(x12_1.shape)\n",
    "        x1p_1 = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
    "        #print(x1p.shape)\n",
    "\n",
    "        # Stage 2\n",
    "        x21_1 = self.do21(F.relu(self.bn21(self.conv21(x1p_1))))\n",
    "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21_1))))\n",
    "        x2p_1 = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31_1 = self.do31(F.relu(self.bn31(self.conv31(x2p_1))))\n",
    "        x32_1 = self.do32(F.relu(self.bn32(self.conv32(x31_1))))\n",
    "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32_1))))\n",
    "        x3p_1 = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41_1 = self.do41(F.relu(self.bn41(self.conv41(x3p_1))))\n",
    "        x42_1 = self.do42(F.relu(self.bn42(self.conv42(x41_1))))\n",
    "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42_1))))\n",
    "        x4p_1 = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
    "\n",
    "        ####################################################\n",
    "        # Stage 1\n",
    "        x11_2 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
    "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11_2))))\n",
    "        x1p_2 = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 2\n",
    "        x21_2 = self.do21(F.relu(self.bn21(self.conv21(x1p_2))))\n",
    "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21_2))))\n",
    "        x2p_2 = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31_2 = self.do31(F.relu(self.bn31(self.conv31(x2p_2))))\n",
    "        x32_2 = self.do32(F.relu(self.bn32(self.conv32(x31_2))))\n",
    "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32_2))))\n",
    "        x3p_2 = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41_2 = self.do41(F.relu(self.bn41(self.conv41(x3p_2))))\n",
    "        x42_2 = self.do42(F.relu(self.bn42(self.conv42(x41_2))))\n",
    "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42_2))))\n",
    "        x4p_2 = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
    "\n",
    "        ####################################################\n",
    "        # Stage 4d\n",
    "        x4d = self.upconv4(x4p_2)\n",
    "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
    "        x4d = torch.cat((pad4(x4d), x43_1, x43_2), 1)\n",
    "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
    "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
    "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
    "\n",
    "        # Stage 3d\n",
    "        x3d = self.upconv3(x41d)\n",
    "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
    "        x3d = torch.cat((pad3(x3d), x33_1, x33_2), 1)\n",
    "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
    "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
    "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
    "\n",
    "        # Stage 2d\n",
    "        x2d = self.upconv2(x31d)\n",
    "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
    "        x2d = torch.cat((pad2(x2d), x22_1, x22_2), 1)\n",
    "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
    "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
    "\n",
    "        # Stage 1d\n",
    "        x1d = self.upconv1(x21d)\n",
    "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
    "        x1d = torch.cat((pad1(x1d), x12_1, x12_2), 1)\n",
    "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
    "        x11d = self.conv11d(x12d)\n",
    "\n",
    "        return self.sm(x11d)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from torchsummary import summary\n",
    "model = SiamUnet_conc(input_nbr=3, label_nbr=2)\n",
    "summary(model,input_size=[(3,256,256),(3,256,256)],batch_size = 2, device=\"cpu\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.padding import ReplicationPad2d\n",
    "\n",
    "\n",
    "class SiamUnet_diff(nn.Module):\n",
    "    \"\"\"SiamUnet_diff segmentation network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_nbr, label_nbr):\n",
    "        super(SiamUnet_diff, self).__init__()\n",
    "\n",
    "        self.input_nbr = input_nbr\n",
    "\n",
    "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(16)\n",
    "        self.do11 = nn.Dropout2d(p=0.2)\n",
    "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(16)\n",
    "        self.do12 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(32)\n",
    "        self.do21 = nn.Dropout2d(p=0.2)\n",
    "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(32)\n",
    "        self.do22 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(64)\n",
    "        self.do31 = nn.Dropout2d(p=0.2)\n",
    "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(64)\n",
    "        self.do32 = nn.Dropout2d(p=0.2)\n",
    "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn33 = nn.BatchNorm2d(64)\n",
    "        self.do33 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(128)\n",
    "        self.do41 = nn.Dropout2d(p=0.2)\n",
    "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(128)\n",
    "        self.do42 = nn.Dropout2d(p=0.2)\n",
    "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn43 = nn.BatchNorm2d(128)\n",
    "        self.do43 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn43d = nn.BatchNorm2d(128)\n",
    "        self.do43d = nn.Dropout2d(p=0.2)\n",
    "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(128)\n",
    "        self.do42d = nn.Dropout2d(p=0.2)\n",
    "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(64)\n",
    "        self.do41d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn33d = nn.BatchNorm2d(64)\n",
    "        self.do33d = nn.Dropout2d(p=0.2)\n",
    "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(64)\n",
    "        self.do32d = nn.Dropout2d(p=0.2)\n",
    "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(32)\n",
    "        self.do31d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(32)\n",
    "        self.do22d = nn.Dropout2d(p=0.2)\n",
    "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(16)\n",
    "        self.do21d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(16)\n",
    "        self.do12d = nn.Dropout2d(p=0.2)\n",
    "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
    "\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"Forward method.\"\"\"\n",
    "        # Stage 1\n",
    "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
    "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
    "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
    "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
    "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
    "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
    "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
    "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
    "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
    "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
    "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
    "\n",
    "        ####################################################\n",
    "        # Stage 1\n",
    "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
    "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
    "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
    "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
    "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
    "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
    "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
    "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
    "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
    "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
    "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4d\n",
    "        x4d = self.upconv4(x4p)\n",
    "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
    "        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n",
    "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
    "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
    "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
    "\n",
    "        # Stage 3d\n",
    "        x3d = self.upconv3(x41d)\n",
    "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
    "        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n",
    "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
    "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
    "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
    "\n",
    "        # Stage 2d\n",
    "        x2d = self.upconv2(x31d)\n",
    "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
    "        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n",
    "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
    "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
    "\n",
    "        # Stage 1d\n",
    "        x1d = self.upconv1(x21d)\n",
    "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
    "        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n",
    "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
    "        x11d = self.conv11d(x12d)\n",
    "\n",
    "        return self.sm(x11d)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from torchsummary import summary\n",
    "model = SiamUnet_diff(input_nbr=3, label_nbr=2)\n",
    "summary(model,input_size=[(3,256,256),(3,256,256)],batch_size = 1, device=\"cpu\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 254, 254])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 16, 256, 256]             880\n",
      "       BatchNorm2d-2          [1, 16, 256, 256]              32\n",
      "         Dropout2d-3          [1, 16, 256, 256]               0\n",
      "            Conv2d-4          [1, 16, 256, 256]           2,320\n",
      "       BatchNorm2d-5          [1, 16, 256, 256]              32\n",
      "         Dropout2d-6          [1, 16, 256, 256]               0\n",
      "            Conv2d-7          [1, 32, 128, 128]           4,640\n",
      "       BatchNorm2d-8          [1, 32, 128, 128]              64\n",
      "         Dropout2d-9          [1, 32, 128, 128]               0\n",
      "           Conv2d-10          [1, 32, 128, 128]           9,248\n",
      "      BatchNorm2d-11          [1, 32, 128, 128]              64\n",
      "        Dropout2d-12          [1, 32, 128, 128]               0\n",
      "           Conv2d-13            [1, 64, 64, 64]          18,496\n",
      "      BatchNorm2d-14            [1, 64, 64, 64]             128\n",
      "        Dropout2d-15            [1, 64, 64, 64]               0\n",
      "           Conv2d-16            [1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-17            [1, 64, 64, 64]             128\n",
      "        Dropout2d-18            [1, 64, 64, 64]               0\n",
      "           Conv2d-19            [1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-20            [1, 64, 64, 64]             128\n",
      "        Dropout2d-21            [1, 64, 64, 64]               0\n",
      "           Conv2d-22           [1, 128, 32, 32]          73,856\n",
      "      BatchNorm2d-23           [1, 128, 32, 32]             256\n",
      "        Dropout2d-24           [1, 128, 32, 32]               0\n",
      "           Conv2d-25           [1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-26           [1, 128, 32, 32]             256\n",
      "        Dropout2d-27           [1, 128, 32, 32]               0\n",
      "           Conv2d-28           [1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-29           [1, 128, 32, 32]             256\n",
      "        Dropout2d-30           [1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-31           [1, 128, 32, 32]         147,584\n",
      "  ConvTranspose2d-32           [1, 128, 32, 32]         295,040\n",
      "      BatchNorm2d-33           [1, 128, 32, 32]             256\n",
      "        Dropout2d-34           [1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-35           [1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-36           [1, 128, 32, 32]             256\n",
      "        Dropout2d-37           [1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-38            [1, 64, 32, 32]          73,792\n",
      "      BatchNorm2d-39            [1, 64, 32, 32]             128\n",
      "        Dropout2d-40            [1, 64, 32, 32]               0\n",
      "  ConvTranspose2d-41            [1, 64, 64, 64]          36,928\n",
      "  ConvTranspose2d-42            [1, 64, 64, 64]          73,792\n",
      "      BatchNorm2d-43            [1, 64, 64, 64]             128\n",
      "        Dropout2d-44            [1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-45            [1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-46            [1, 64, 64, 64]             128\n",
      "        Dropout2d-47            [1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-48            [1, 32, 64, 64]          18,464\n",
      "      BatchNorm2d-49            [1, 32, 64, 64]              64\n",
      "        Dropout2d-50            [1, 32, 64, 64]               0\n",
      "  ConvTranspose2d-51          [1, 32, 128, 128]           9,248\n",
      "  ConvTranspose2d-52          [1, 32, 128, 128]          18,464\n",
      "      BatchNorm2d-53          [1, 32, 128, 128]              64\n",
      "        Dropout2d-54          [1, 32, 128, 128]               0\n",
      "  ConvTranspose2d-55          [1, 16, 128, 128]           4,624\n",
      "      BatchNorm2d-56          [1, 16, 128, 128]              32\n",
      "        Dropout2d-57          [1, 16, 128, 128]               0\n",
      "  ConvTranspose2d-58          [1, 16, 256, 256]           2,320\n",
      "  ConvTranspose2d-59          [1, 16, 256, 256]           4,624\n",
      "      BatchNorm2d-60          [1, 16, 256, 256]              32\n",
      "        Dropout2d-61          [1, 16, 256, 256]               0\n",
      "  ConvTranspose2d-62           [1, 2, 254, 254]              34\n",
      "================================================================\n",
      "Total params: 1,350,322\n",
      "Trainable params: 1,350,322\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 179.48\n",
      "Params size (MB): 5.15\n",
      "Estimated Total Size (MB): 184.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.padding import ReplicationPad2d\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"EF segmentation network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_nbr, label_nbr):\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        self.input_nbr = input_nbr\n",
    "\n",
    "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(16)\n",
    "        self.do11 = nn.Dropout2d(p=0.2)\n",
    "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(16)\n",
    "        self.do12 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(32)\n",
    "        self.do21 = nn.Dropout2d(p=0.2)\n",
    "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(32)\n",
    "        self.do22 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(64)\n",
    "        self.do31 = nn.Dropout2d(p=0.2)\n",
    "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(64)\n",
    "        self.do32 = nn.Dropout2d(p=0.2)\n",
    "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn33 = nn.BatchNorm2d(64)\n",
    "        self.do33 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(128)\n",
    "        self.do41 = nn.Dropout2d(p=0.2)\n",
    "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(128)\n",
    "        self.do42 = nn.Dropout2d(p=0.2)\n",
    "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn43 = nn.BatchNorm2d(128)\n",
    "        self.do43 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn43d = nn.BatchNorm2d(128)\n",
    "        self.do43d = nn.Dropout2d(p=0.2)\n",
    "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(128)\n",
    "        self.do42d = nn.Dropout2d(p=0.2)\n",
    "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(64)\n",
    "        self.do41d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn33d = nn.BatchNorm2d(64)\n",
    "        self.do33d = nn.Dropout2d(p=0.2)\n",
    "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(64)\n",
    "        self.do32d = nn.Dropout2d(p=0.2)\n",
    "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(32)\n",
    "        self.do31d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(32)\n",
    "        self.do22d = nn.Dropout2d(p=0.2)\n",
    "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(16)\n",
    "        self.do21d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(16)\n",
    "        self.do12d = nn.Dropout2d(p=0.2)\n",
    "        # self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=1, padding=1)\n",
    "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
    "        #self.sm = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        \"\"\"Forward method.\"\"\"\n",
    "        # Stage 1\n",
    "        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n",
    "        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
    "        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
    "        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
    "        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
    "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
    "        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
    "        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
    "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
    "        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
    "        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4d\n",
    "        x4d = self.upconv4(x4p)\n",
    "        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n",
    "        x4d = torch.cat((pad4(x4d), x43), 1)\n",
    "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
    "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
    "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
    "\n",
    "        # Stage 3d\n",
    "        x3d = self.upconv3(x41d)\n",
    "        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n",
    "        x3d = torch.cat((pad3(x3d), x33), 1)\n",
    "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
    "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
    "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
    "\n",
    "        # Stage 2d\n",
    "        x2d = self.upconv2(x31d)\n",
    "        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n",
    "        x2d = torch.cat((pad2(x2d), x22), 1)\n",
    "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
    "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
    "\n",
    "        # Stage 1d\n",
    "        x1d = self.upconv1(x21d)\n",
    "        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n",
    "        x1d = torch.cat((pad1(x1d), x12), 1)\n",
    "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
    "        x11d = self.conv11d(x12d)\n",
    "        x = torch.sigmoid(x11d)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "model = Unet(input_nbr=6, label_nbr=2)\n",
    "summary(model,input_size=[(3,256,256),(3,256,256)],batch_size = 1, device=\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, relu_first=True,\n",
    "                 bias=False, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        depthwise = nn.Conv2d(inplanes, inplanes, kernel_size,\n",
    "                              stride=stride, padding=dilation,\n",
    "                              dilation=dilation, groups=inplanes, bias=bias)\n",
    "        bn_depth = norm_layer(inplanes)\n",
    "        pointwise = nn.Conv2d(inplanes, planes, 1, bias=bias)\n",
    "        bn_point = norm_layer(planes)\n",
    "\n",
    "        if relu_first:\n",
    "            self.block = nn.Sequential(OrderedDict([('relu', nn.ReLU()),\n",
    "                                                    ('depthwise', depthwise),\n",
    "                                                    ('bn_depth', bn_depth),\n",
    "                                                    ('pointwise', pointwise),\n",
    "                                                    ('bn_point', bn_point)\n",
    "                                                    ]))\n",
    "        else:\n",
    "            self.block = nn.Sequential(OrderedDict([('depthwise', depthwise),\n",
    "                                                    ('bn_depth', bn_depth),\n",
    "                                                    ('relu1', nn.ReLU(inplace=True)),\n",
    "                                                    ('pointwise', pointwise),\n",
    "                                                    ('bn_point', bn_point),\n",
    "                                                    ('relu2', nn.ReLU(inplace=True))\n",
    "                                                    ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, dilation=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=3, dilation=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class _ASPP(nn.Module):\n",
    "    def __init__(self, in_channels=2048, out_channels=256):\n",
    "        super().__init__()\n",
    "\n",
    "        dilations = [6, 12, 18]\n",
    "\n",
    "        self.aspp0 = nn.Sequential(OrderedDict([('conv', nn.Conv2d(in_channels, out_channels, 1, bias=False)),\n",
    "                                                ('bn', nn.BatchNorm2d(out_channels)),\n",
    "                                                ('relu', nn.ReLU(inplace=True))]))\n",
    "        self.aspp1 = SeparableConv2d(in_channels, out_channels, dilation=dilations[0], relu_first=False)\n",
    "        self.aspp2 = SeparableConv2d(in_channels, out_channels, dilation=dilations[1], relu_first=False)\n",
    "        self.aspp3 = SeparableConv2d(in_channels, out_channels, dilation=dilations[2], relu_first=False)\n",
    "\n",
    "        self.image_pooling = nn.Sequential(OrderedDict([('gap', nn.AdaptiveAvgPool2d((1, 1))),\n",
    "                                                        ('conv', nn.Conv2d(in_channels, out_channels, 1, bias=False)),\n",
    "                                                        ('bn', nn.BatchNorm2d(out_channels)),\n",
    "                                                        ('relu', nn.ReLU(inplace=True))]))\n",
    "\n",
    "        self.conv = nn.Conv2d(out_channels * 5, out_channels, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pool = self.image_pooling(x)\n",
    "        pool = F.interpolate(pool, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        x0 = self.aspp0(x)\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x = torch.cat((pool, x0, x1, x2, x3), dim=1)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet_ASPP(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet_ASPP, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.aspp = _ASPP(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x_1, x_2):\n",
    "        #print(x.shape)\n",
    "        x = torch.cat((x_1, x_2), dim=1)\n",
    "        x1 = self.inc(x)\n",
    "        #print(x1.shape)\n",
    "        x2 = self.down1(x1)\n",
    "        #print(x2.shape)\n",
    "        x3 = self.down2(x2)\n",
    "        #print(x3.shape)\n",
    "        x4 = self.down3(x3)\n",
    "        #print(x4.shape)\n",
    "        x5 = self.aspp(x4)\n",
    "        #print(x5.shape)\n",
    "        x = self.up1(x5, x4)\n",
    "        #print(x.shape)\n",
    "        x = self.up2(x, x3)\n",
    "        #print(x.shape)\n",
    "        x = self.up3(x, x2)\n",
    "        #print(x.shape)\n",
    "        x = self.up4(x, x1)\n",
    "        #print(x.shape)\n",
    "        x = self.outc(x)\n",
    "        #print(x.shape)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from torchsummary import summary\n",
    "model = UNet_ASPP(n_channels=6,n_classes=2)\n",
    "summary(model,input_size=[(3,256,256),(3,256,256)],batch_size = 2, device=\"cpu\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
