{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3671e5-48b6-41a4-b828-6a1adf608842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T08:38:26.473909Z",
     "iopub.status.busy": "2023-02-25T08:38:26.473541Z",
     "iopub.status.idle": "2023-02-25T08:38:31.903269Z",
     "shell.execute_reply": "2023-02-25T08:38:31.902219Z",
     "shell.execute_reply.started": "2023-02-25T08:38:26.473823Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting change-detection-pytorch\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/aa/16/39b93e49a0bb5fe055d7ed4c70a098c92407fdfbcd9331d687d81a762d8a/change_detection_pytorch-0.1.4-py3-none-any.whl (137 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m137.8/137.8 kB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting timm==0.4.12\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/90/fc/606bc5cf46acac3aa9bd179b3954433c026aaf88ea98d6b19f5d14c336da/timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m377.0/377.0 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.8/site-packages (from change-detection-pytorch) (0.9.2+cu111)\n",
      "Collecting pretrainedmodels==0.7.4\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.8/58.8 kB\u001B[0m \u001B[31m220.1 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting efficientnet-pytorch==0.6.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b8/cb/0309a6e3d404862ae4bc017f89645cf150ac94c14c88ef81d215c8e52925/efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting albumentations\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4f/55/3c2ce84c108fc1d422afd6de153e4b0a3e6f96ecec4cb9afcf0284ce3538/albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m123.5/123.5 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/site-packages (from efficientnet-pytorch==0.6.3->change-detection-pytorch) (1.8.2+cu111)\n",
      "Collecting munch\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->change-detection-pytorch) (4.64.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from torchvision>=0.5.0->change-detection-pytorch) (1.23.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/site-packages (from torchvision>=0.5.0->change-detection-pytorch) (9.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch->efficientnet-pytorch==0.6.3->change-detection-pytorch) (4.4.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.8/site-packages (from albumentations->change-detection-pytorch) (4.5.3.100)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/site-packages (from albumentations->change-detection-pytorch) (1.9.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/site-packages (from albumentations->change-detection-pytorch) (6.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/site-packages (from albumentations->change-detection-pytorch) (0.19.3)\n",
      "Collecting qudida>=0.0.4\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f0/a1/a5f4bebaa31d109003909809d88aeb0d4b201463a9ea29308d9e4f9e7655/qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.8/site-packages (from qudida>=0.0.4->albumentations->change-detection-pytorch) (1.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations->change-detection-pytorch) (21.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations->change-detection-pytorch) (2022.10.10)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations->change-detection-pytorch) (2.22.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations->change-detection-pytorch) (1.4.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations->change-detection-pytorch) (2.8.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from munch->pretrainedmodels==0.7.4->change-detection-pytorch) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations->change-detection-pytorch) (3.0.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->change-detection-pytorch) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->change-detection-pytorch) (1.2.0)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12403 sha256=18dedc94c7a0ca6c03e3f92f6be4cb36d77557824b093f2bdad799bde9c5de48\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/3f/1a/183399ebaa5998d3e4d60780ca5951bbeab4e6b69cc3fea227\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=8a1cc87535d8459354c43fea6f852a1340e26c4b232bfef773f65e46c0f76887\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/32/54/89d6b8e7e710086237f700ad61d8b3d8af521e5e5d9b3d632c\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, qudida, pretrainedmodels, albumentations, change-detection-pytorch\n",
      "Successfully installed albumentations-1.3.0 change-detection-pytorch-0.1.4 efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 qudida-0.0.4 timm-0.4.12\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install change-detection-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf4b46cc-2288-41d4-8f47-c581edc0c459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T08:38:31.905913Z",
     "iopub.status.busy": "2023-02-25T08:38:31.905605Z",
     "iopub.status.idle": "2023-02-25T08:50:16.193576Z",
     "shell.execute_reply": "2023-02-25T08:50:16.192957Z",
     "shell.execute_reply.started": "2023-02-25T08:38:31.905891Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/usr/local/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3800 images\n",
      "Loaded 200 images\n",
      "\n",
      "Epoch: 0\n",
      "train: 100%|██████████| 475/475 [01:30<00:00,  5.23it/s, cross_entropy_loss - 0.4152, fscore - 0.6104, precision - 0.6136, recall - 0.6722]\n",
      "valid: 100%|██████████| 200/200 [00:05<00:00, 36.68it/s, cross_entropy_loss - 0.2817, fscore - 0.5273, precision - 0.6862, recall - 0.5211]\n",
      "max_score 0.5272537768339336\n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|██████████| 475/475 [01:07<00:00,  7.03it/s, cross_entropy_loss - 0.2608, fscore - 0.7387, precision - 0.7609, recall - 0.7431]\n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 54.14it/s, cross_entropy_loss - 0.2352, fscore - 0.5855, precision - 0.6811, recall - 0.6002]\n",
      "max_score 0.585503879279907\n",
      "Model saved!\n",
      "\n",
      "Epoch: 2\n",
      "train: 100%|██████████| 475/475 [01:02<00:00,  7.65it/s, cross_entropy_loss - 0.2093, fscore - 0.7905, precision - 0.8055, recall - 0.7968]\n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 53.96it/s, cross_entropy_loss - 0.2029, fscore - 0.5886, precision - 0.7853, recall - 0.5668]\n",
      "max_score 0.5885864456110284\n",
      "Model saved!\n",
      "\n",
      "Epoch: 3\n",
      "train: 100%|██████████| 475/475 [01:02<00:00,  7.65it/s, cross_entropy_loss - 0.1656, fscore - 0.8383, precision - 0.8489, recall - 0.839] \n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 53.80it/s, cross_entropy_loss - 0.2077, fscore - 0.6106, precision - 0.7704, recall - 0.6145]\n",
      "max_score 0.6105639670986875\n",
      "Model saved!\n",
      "\n",
      "Epoch: 4\n",
      "train: 100%|██████████| 475/475 [01:02<00:00,  7.65it/s, cross_entropy_loss - 0.1386, fscore - 0.8632, precision - 0.8723, recall - 0.863] \n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 54.11it/s, cross_entropy_loss - 0.1726, fscore - 0.641, precision - 0.7809, recall - 0.6505] \n",
      "max_score 0.6409697138680258\n",
      "Model saved!\n",
      "\n",
      "Epoch: 5\n",
      "train: 100%|██████████| 475/475 [01:02<00:00,  7.65it/s, cross_entropy_loss - 0.1217, fscore - 0.8806, precision - 0.8864, recall - 0.8818]\n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 53.86it/s, cross_entropy_loss - 0.1701, fscore - 0.6578, precision - 0.8027, recall - 0.659] \n",
      "max_score 0.6577653647163862\n",
      "Model saved!\n",
      "\n",
      "Epoch: 6\n",
      "train: 100%|██████████| 475/475 [01:02<00:00,  7.64it/s, cross_entropy_loss - 0.105, fscore - 0.8961, precision - 0.9013, recall - 0.8956]  \n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 54.27it/s, cross_entropy_loss - 0.1652, fscore - 0.6702, precision - 0.7438, recall - 0.7077]\n",
      "max_score 0.6702029467846867\n",
      "Model saved!\n",
      "\n",
      "Epoch: 7\n",
      "train: 100%|██████████| 475/475 [01:02<00:00,  7.64it/s, cross_entropy_loss - 0.09862, fscore - 0.9019, precision - 0.9059, recall - 0.9017]\n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 53.98it/s, cross_entropy_loss - 0.167, fscore - 0.6971, precision - 0.771, recall - 0.7409]  \n",
      "max_score 0.6971310454818509\n",
      "Model saved!\n",
      "\n",
      "Epoch: 8\n",
      "train: 100%|██████████| 475/475 [01:01<00:00,  7.67it/s, cross_entropy_loss - 0.08638, fscore - 0.9113, precision - 0.9172, recall - 0.9086]\n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 54.11it/s, cross_entropy_loss - 0.151, fscore - 0.6911, precision - 0.7996, recall - 0.7059] \n",
      "\n",
      "Epoch: 9\n",
      "train: 100%|██████████| 475/475 [01:01<00:00,  7.68it/s, cross_entropy_loss - 0.07129, fscore - 0.9262, precision - 0.9311, recall - 0.9233]\n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 54.32it/s, cross_entropy_loss - 0.145, fscore - 0.7262, precision - 0.7716, recall - 0.7527] \n",
      "max_score 0.7262054100406463\n",
      "Model saved!\n",
      "valid: 100%|██████████| 200/200 [00:03<00:00, 55.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import change_detection_pytorch as cdp\n",
    "from change_detection_pytorch.datasets import CustomDataset, LEVIR_CD_Dataset, Dataset\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = cdp.Unet(\n",
    "    encoder_name=\"resnet34\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=2,  # model output channels (number of classes in your datasets)\n",
    "    siam_encoder=True,  # whether to use a siamese encoder\n",
    "    fusion_form='concat',  # the form of fusing features from two branches. e.g. concat, sum, diff, or abs_diff.\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = LEVIR_CD_Dataset('data/train',\n",
    "                                 sub_dir_1='time1',\n",
    "                                 sub_dir_2='time2',\n",
    "                                 img_suffix='.png',\n",
    "                                 ann_dir='data/train/label',\n",
    "                                 debug=False)\n",
    "\n",
    "valid_dataset = LEVIR_CD_Dataset('data/val',\n",
    "                                 sub_dir_1='time1',\n",
    "                                 sub_dir_2='time2',\n",
    "                                 img_suffix='.png',\n",
    "                                 ann_dir='data/val/label',\n",
    "                                 debug=False,\n",
    "                                 test_mode=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "loss = cdp.utils.losses.CrossEntropyLoss()\n",
    "metrics = [\n",
    "    cdp.utils.metrics.Precision(activation='argmax2d'),\n",
    "    cdp.utils.metrics.Recall(activation='argmax2d'),\n",
    "    cdp.utils.metrics.Fscore(activation='argmax2d'),\n",
    "    cdp.utils.metrics.IoU(activation=\"argmax2d\")\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])\n",
    "\n",
    "scheduler_steplr = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, ], gamma=0.1)\n",
    "\n",
    "# create epoch runners\n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = cdp.utils.train.TrainEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = cdp.utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# train model for 60 epochs\n",
    "\n",
    "max_score = 0\n",
    "MAX_EPOCH = 10\n",
    "\n",
    "for i in range(MAX_EPOCH):\n",
    "\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    scheduler_steplr.step()\n",
    "\n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['fscore']:\n",
    "        max_score = valid_logs['fscore']\n",
    "        print('max_score', max_score)\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')\n",
    "\n",
    "# save results (change maps)\n",
    "\"\"\"\n",
    "Note: if you use sliding window inference, set:\n",
    "    from change_detection_pytorch.datasets.transforms.albu import (\n",
    "        ChunkImage, ToTensorTest)\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "        A.Normalize(),\n",
    "        ChunkImage({window_size}}),\n",
    "        ToTensorTest(),\n",
    "    ], additional_targets={'image_2': 'image'})\n",
    "\n",
    "\"\"\"\n",
    "valid_epoch.infer_vis(valid_loader, save=True, slide=False, save_dir='res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe5553-7d42-4cd6-89b3-2092c46d0852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
